/*
YogiMPI Library - MPI ABI Translator
Copyright (C) 2006, 2007 Toon Knapen Free Field Technologies S.A.
Additions made by Stephen Adamec, University of Alabama at Birmingham

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
*/

#include <yogimpi.h>
#include <mpi.h>
#include <assert.h>
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <unistd.h> /* for hostname */

/* Total number of non-volatile datatypes.  Accounts for Fortran ones, too. */
static const int YogiMPI_DATATYPE_VOLATILE_OFFSET = 38;

/* Total number of datatypes, volatile and non-volatile. 
 */
static int num_datatypes = 100; 

/* Now have a pointer to a pool of MPI datatypes, handing references to
 * users without exposing the opaque MPI handle.
 */
static MPI_Datatype *datatype_pool = NULL;

/* Total number of times datatype pool was resized. Initially zero. */
static int num_realloc_datatypes = 0;

/* Allocates storage and indexes a new volatile MPI datatype in the YogiMPI 
 * pool.  
 * @param mpi_datatype An actual MPI_Datatype to be archived and indexed.
 * @return YogiMPI_Datatype handle to the actual MPI_Datatype.
 */
static YogiMPI_Datatype add_new_datatype(MPI_Datatype mpi_datatype)
{

  /* Find the next available slot to store a new MPI_Datatype.
   * If there aren't enough slots, increase the number of slots. */
  YogiMPI_Datatype new_slot = YogiMPI_DATATYPE_VOLATILE_OFFSET;
  for( ; datatype_pool[new_slot] != MPI_DATATYPE_NULL && 
         new_slot < num_datatypes; ++new_slot );

  if (new_slot == num_datatypes) {
	/* No space left, so double the array size. */
    int new_num_datatypes = num_datatypes * 2;
    ++num_realloc_datatypes;
    int new_pool_size = new_num_datatypes * sizeof(MPI_Datatype);
    MPI_Datatype *new_datatypes = (MPI_Datatype *)realloc(datatype_pool,
    		                                             new_pool_size);

    /* Fill in all the newly-formed slots with NULL values. */
    int i;
    for(i = num_datatypes; i < new_num_datatypes; ++i) {
    	new_datatypes[i] = MPI_DATATYPE_NULL;
    }

    /* Reassign old pool pointer and counter to larger space and counter. */
    datatype_pool = new_datatypes;
    num_datatypes = new_num_datatypes;
  }
  
  datatype_pool[new_slot] = mpi_datatype;
  return new_slot;
  
}

/* Holds rank of this MPI process */
int my_rank = -1;

/* Array maps YogiMPI error constants to actual MPI constants */
static int mpi_error_codes[37];

static int num_groups = 100; 
/* Pointer to pool of MPI_Group objects */
static MPI_Group *group_pool = NULL;
/* Number of times group pool is reallocated for expansion. */
static int num_realloc_groups = 0; 
/* from this offset onward up to num_groups are volatile in group_pool */
static const YogiMPI_Group YogiMPI_GROUP_VOLATILE_OFFSET = 2; 

/* Pointer to pool of MPI_Comm objects */
static MPI_Comm *comm_pool = NULL;
static int num_comms = 100; 
/* Number of times comm pool is reallocated for expansion. */
static int num_realloc_comms = 0;
/* From this offset onward up to num_comms the communicators in comm_pool are
   volatile */
static const YogiMPI_Comm YogiMPI_COMM_VOLATILE_OFFSET = 3;

static int num_requests = 100;
/* Pointer to pool of MPI_Request objects */
static MPI_Request *request_pool = NULL;
/* Number of times request pool is reallocated for expansion. */
static int num_realloc_requests = 0;

static int num_ops = 20;
/* Pointer to pool of MPI_Op objects */
static MPI_Op *op_pool = NULL;
/* From this offset onward up to num_ops the Op objects in op_pool are
   volatile */
static const YogiMPI_Op YogiMPI_OP_VOLATILE_OFFSET = 13;

/* Do the same thing for MPI_Info and MPI_File pools. */
static int num_files = 100;
static int num_infos = 100;
/* Pointer to pool of MPI_File objects */
static MPI_File *file_pool = NULL;
/* Pointer to pool of MPI_Info objects */
static MPI_Info *info_pool = NULL;
/* From this offset onward up to num_files the File objects in file_pool are
   volatile */
static const YogiMPI_File YogiMPI_FILE_VOLATILE_OFFSET = 1;
/* From this offset onward up to num_infos the Info objects in info_pool are
   volatile */
static const YogiMPI_Info YogiMPI_INFO_VOLATILE_OFFSET = 1;

/* Internal definitions for MPI_Errhandler */
static int num_errhandlers = 20;
/* Pointer to pool of MPI_Errhandler objects */
static MPI_Errhandler *errhandler_pool = NULL;
/* From this offset onward up to num_errhandlers MPI_Errhandlers are
   volatile */
static const YogiMPI_Errhandler YogiMPI_ERRHANDLER_VOLATILE_OFFSET = 3;

/*
 * conversion functions YogiMPI <-> MPI
 */

/* Converts an MPI-error into a YogiMPI-error */
int error_to_yogi(int mpi_error) 
{ 
  int current = 0;
  while(mpi_error_codes[current] != mpi_error) {
    if (current > YogiMPI_ERR_LASTCODE) {
      return YogiMPI_ERR_INTERN;
    }
    ++current;
  }
  return current;
}

/* Convert a YogiMPI-error to MPI-error */
int yogi_error_to_mpi(int yogi_error) {
  return mpi_error_codes[yogi_error];
}

/* Converts a YogiMPI_Datatype to MPI_Datatype */
MPI_Datatype datatype_to_mpi(YogiMPI_Datatype in)
{ 
  return datatype_pool[in]; 
}

/* Converts a YogiMPI_Group to MPI_Group */
MPI_Group group_to_mpi(YogiMPI_Group group)
{
  return group_pool[group];
}

/* Converts a YogiMPI_Comm to MPI_Comm */
MPI_Comm comm_to_mpi(YogiMPI_Comm comm)
{
  return comm_pool[comm];
}

/* Converts a YogiMPI_Comm array to an array of MPI_Comm objects */ 
MPI_Comm * comm_array_to_mpi(YogiMPI_Comm array_of_comms[], int num) {
    MPI_Comm* mpi_comms = (MPI_Comm*)malloc(num*sizeof(MPI_Comm));
    int i;
    for(i = 0; i < num; ++i) {
        mpi_comms[i] = comm_to_mpi(array_of_comms[i]);
    }
    return mpi_comms;
}

/* Frees an array of MPI Comm objects */
void free_comm_array(MPI_Comm array_of_comms[]) {
    free(array_of_comms);
}

/* Converts a YogiMPI_Datatype array to an array of MPI_Datatype objects */
MPI_Datatype * datatype_array_to_mpi(YogiMPI_Datatype array_of_types[],
                                     int num) {
    MPI_Datatype* mpi_types = (MPI_Datatype*)malloc(num*sizeof(MPI_Datatype));
    int i;
    for(i = 0; i < num; ++i) {
        mpi_types[i] = datatype_to_mpi(array_of_types[i]);
    }
    return mpi_types;
}

/* Frees an array of MPI_Datatype objects */
void free_datatype_array(MPI_Datatype *array_of_types) {
	if (array_of_types != NULL) {
	    free(array_of_types);
	}
}

/* Converts a YogiMPI_Request to MPI_Request pointer */
MPI_Request* request_to_mpi(YogiMPI_Request request) 
{
    return &request_pool[request];
}

/* Converts a YogiMPI_Op to MPI_Op */
MPI_Op op_to_mpi(YogiMPI_Op op)
{ 
    return op_pool[op]; 
}

/* Converts a YogiMPI_File to MPI_File */
MPI_File file_to_mpi(YogiMPI_File file)
{ 
    return file_pool[file]; 
}

/* Converts a YogiMPI_Info to MPI_Info */
MPI_Info info_to_mpi(YogiMPI_Info info)
{ 
    return info_pool[info]; 
}

/* Converts a YogiMPI_Errhandler to MPI_Errhandler */
MPI_Errhandler errhandler_to_mpi(YogiMPI_Errhandler errhandler) {
	return errhandler_pool[errhandler];
}

/* Convert MPI comparison constants to YogiMPI comparison constants */
int comparison_to_yogi(int mpi_comp)
{
  if (mpi_comp == MPI_IDENT) return YogiMPI_IDENT;
  if (mpi_comp == MPI_CONGRUENT) return YogiMPI_CONGRUENT;
  if (mpi_comp == MPI_SIMILAR) return YogiMPI_SIMILAR;
  if (mpi_comp == MPI_UNEQUAL) return YogiMPI_UNEQUAL;
  return YogiMPI_UNEQUAL;
}

/* Register pre-indexed group */
static void register_preindexed_group(YogiMPI_Group group, MPI_Group mpi_group) 
{
  assert(group >= 0 && group < YogiMPI_GROUP_VOLATILE_OFFSET);
  group_pool[group] = mpi_group;
}

/* Allocate a new YogiMPI_Group that corresponds to the given MPI_Group
 *
 * 1) Find an available 'index' for a new YogiMPI_Group
 * 2) Save the MPI_Group at the index 
 * 3) Set the ref-count to 1
 * 
 * This routine is only used for volatile groups because the others have
 * a predefined index. Thus mpi_group must thus also be a volatile group.
 */
static YogiMPI_Group add_new_group(MPI_Group mpi_group)
{

    /* find new slot */
    YogiMPI_Group new_slot = YogiMPI_GROUP_VOLATILE_OFFSET;
    for(; group_pool[new_slot] != MPI_GROUP_NULL && 
          new_slot < num_groups; ++new_slot);

    /* if not sufficient slots, increase number of slots */
    if (new_slot == num_groups) {
        int new_num_groups = num_groups * 2;
        int i;

        ++num_realloc_groups; /* update stats */

        /* realloc group_pool */
        MPI_Group* new_groups = (MPI_Group*)malloc(new_num_groups*sizeof(MPI_Group));
        memcpy(new_groups,group_pool,num_groups*sizeof(MPI_Group)) ;
        free(group_pool);

        for(i = num_groups; i < new_num_groups; ++i) {
            new_groups[i] = MPI_GROUP_NULL;
        }

        group_pool = new_groups;
        num_groups = new_num_groups;
    }

    assert(new_slot < num_groups);
    assert(group_pool[new_slot] == MPI_GROUP_NULL);
    group_pool[new_slot] = mpi_group;

    return new_slot;
}

static void register_preindexed_comm(YogiMPI_Comm comm, MPI_Comm mpi_comm) {
    assert(comm >= 0 && comm < num_comms);
    comm_pool[comm] = mpi_comm;
}

static YogiMPI_Comm add_new_comm(MPI_Comm mpi_comm) {

    /* find new slot */
    YogiMPI_Comm new_slot = YogiMPI_COMM_VOLATILE_OFFSET;
    for(; comm_pool[new_slot] != MPI_COMM_NULL &&
        new_slot < num_comms; ++new_slot);

    /* if not sufficient slots, increase number of slots */
    if (new_slot == num_comms) {
        int new_num_comms = num_comms * 2;
        int i;

        ++num_realloc_comms; /* update stats */

        /* realloc comms_, comm_groups_ and is_inter_comms_ */
        MPI_Comm* new_comms = (MPI_Comm*)malloc(new_num_comms*sizeof(MPI_Comm));
        memcpy(new_comms,comm_pool,num_comms*sizeof(MPI_Comm));
        free(comm_pool);

        for(i = num_comms; i < new_num_comms; ++i) new_comms[i] = MPI_COMM_NULL;

        comm_pool = new_comms;
        num_comms = new_num_comms;
    }

    assert(comm_pool[new_slot] == MPI_COMM_NULL);
    comm_pool[new_slot] = mpi_comm;

    return new_slot;
}

static YogiMPI_Request add_new_request(MPI_Request mpi_request)
{
    /* Find a slot, start from 1 because 0 == YogiMPI_REQUEST_NULL */
    YogiMPI_Request request = 1; 
    for(; request < num_requests && request_pool[request] != MPI_REQUEST_NULL;
    	++request);

    /* if not sufficient slots, increase number of slots */
    if (request == num_requests) {
        int new_num_requests = num_requests * 2;
        ++num_realloc_requests;

        /* realloc */
        MPI_Request* new_requests = (MPI_Request*)malloc(new_num_requests*sizeof(MPI_Request));
        memcpy(new_requests,request_pool,num_requests*sizeof(MPI_Request));
        free(request_pool);

        int i;
        for(i = num_requests; i < new_num_requests; ++i) {
        	new_requests[i] = MPI_REQUEST_NULL;
        }

        request_pool = new_requests;
        num_requests = new_num_requests;
    }

    assert(request < num_requests);
    assert(request_pool[request] == MPI_REQUEST_NULL);
    request_pool[request] = mpi_request;

    return request;
}

static YogiMPI_File add_new_file(MPI_File mpi_file)
{
    /* Find a slot, start from 1 because 0 == YogiMPI_FILE_NULL */
    YogiMPI_File file = 1; 
    for(; file < num_files && file_pool[file] != MPI_FILE_NULL; ++file);

    /* if not sufficient slots, increase number of slots */
    if (file == num_files) {
        int new_num_files = num_files * 2;

        /* realloc */
        MPI_File* new_files = (MPI_File*)malloc(new_num_files*sizeof(MPI_File));
        memcpy(new_files,file_pool,num_files*sizeof(MPI_File));
        free(file_pool);

        int i;
        for(i = num_files; i < new_num_files; ++i) {
        	new_files[i] = MPI_FILE_NULL;
        }

        file_pool = new_files;
        num_files = new_num_files;
    }

    assert(file < num_files);
    assert(file_pool[file] == MPI_FILE_NULL);
    file_pool[file] = mpi_file;

    return file;
}

static YogiMPI_Info add_new_info(MPI_Info mpi_info)
{
    /* Find a slot, start from 1 because 0 == YogiMPI_INFO_NULL */
    YogiMPI_Info info = 1; 
    for(; info < num_infos && info_pool[info] != MPI_INFO_NULL; ++info);

    /* if not sufficient slots, increase number of slots */
    if (info == num_infos) {
        int new_num_infos = num_infos * 2;

        /* realloc */
        MPI_Info* new_infos = (MPI_Info*)malloc(new_num_infos*sizeof(MPI_Info));
        memcpy(new_infos,info_pool,num_infos*sizeof(MPI_Info));
        free(info_pool);

        int i;
        for(i = num_infos; i < new_num_infos; ++i) {
        	new_infos[i] = MPI_INFO_NULL;
        }

        info_pool = new_infos;
        num_infos = new_num_infos;
    }

    assert(info < num_infos);
    assert(info_pool[info] == MPI_INFO_NULL);
    info_pool[info] = mpi_info;

    return info;
}

static YogiMPI_Errhandler add_new_errhandler(MPI_Errhandler mpi_errhandler)
{
    int i; 
    for(i = YogiMPI_ERRHANDLER_VOLATILE_OFFSET; i < num_errhandlers &&
        errhandler_pool[i] != MPI_ERRHANDLER_NULL; ++i);

    /* if not sufficient slots, increase number of slots */
    if (i == num_errhandlers) {
        int new_num_errhandlers = num_errhandlers * 2;

        /* realloc */
        MPI_Errhandler* new_errhandlers = (MPI_Errhandler*)malloc(new_num_errhandlers
        		                          *sizeof(MPI_Errhandler));
        memcpy(new_errhandlers,errhandler_pool,num_errhandlers*sizeof(MPI_Errhandler));
        free(errhandler_pool);

        int j;
        for(j = num_errhandlers; j < new_num_errhandlers; ++j) {
        	new_errhandlers[j] = MPI_ERRHANDLER_NULL;
        }

        errhandler_pool = new_errhandlers;
        num_errhandlers = new_num_errhandlers;
    }

    assert(i < num_errhandlers);
    assert(errhandler_pool[i] == MPI_ERRHANDLER_NULL);
    errhandler_pool[i] = mpi_errhandler;

    return i;
}

static YogiMPI_Op add_new_op(MPI_Op mpi_op)
{
    int i; 
    for(i = YogiMPI_OP_VOLATILE_OFFSET; i < num_ops &&
        op_pool[i] != MPI_OP_NULL; ++i);

    /* if not sufficient slots, increase number of slots */
    if (i == num_ops) {
        int new_num_ops = num_ops * 2;

        /* realloc */
        MPI_Op* new_ops = (MPI_Op*)malloc(new_num_ops*sizeof(MPI_Op));
        memcpy(new_ops, op_pool, num_ops*sizeof(MPI_Op));
        free(op_pool);

        int j;
        for(j = num_ops; j < new_num_ops; ++j) {
        	new_ops[j] = MPI_OP_NULL;
        }

        op_pool = new_ops;
        num_ops = new_num_ops;
    }

    assert(i < num_ops);
    assert(op_pool[i] == MPI_OP_NULL);
    op_pool[i] = mpi_op;

    return i;
}

/* Copy an MPI_Status pointer into a YogiMPI_Status object.
 * @arg source The MPI_Status memory address from which to copy.
 * @arg dest The YogiMPI_Status memory address into which copy is placed.
 */
static void mpi_status_to_yogi(MPI_Status *source, YogiMPI_Status *dest) {
    dest->MPI_TAG = source->MPI_TAG;
    dest->MPI_SOURCE = source->MPI_SOURCE;
    dest->MPI_ERROR = source->MPI_ERROR;
    /* If this isn't the same address, force a memcpy */
    if ((void *)dest->realStatus != (void *)source) {
        memcpy((void *)dest->realStatus, (void *)source, sizeof(MPI_Status));
    }
}

/* Retrieve the real MPI_Status pointer from a YogiMPI_Status object */
static MPI_Status * yogi_status_to_mpi(YogiMPI_Status *source)
{
    /* This will grab the number of bytes needed.  We don't care about
     * structure padding since this area is never directly accessed by us.
     * It is ensured to be larger than we need.
    */
    return (MPI_Status *)&source->realStatus[0];
}

static MPI_Offset offset_to_mpi(YogiMPI_Offset in) {
    return (MPI_Offset)	in;
}

static MPI_Aint aint_to_mpi(YogiMPI_Aint in) {
	return (MPI_Aint) in;
}

static YogiMPI_Aint aint_to_yogi(MPI_Aint in) {
	return (YogiMPI_Aint) in;
}

static YogiMPI_Offset offset_to_yogi(MPI_Offset in) {
	return (YogiMPI_Offset) in;
}

static int yogi_amode_to_mpi(int amode) {
    switch(amode) {
    case YogiMPI_MODE_RDONLY: 
	    return MPI_MODE_RDONLY;
	    break;
    case YogiMPI_MODE_RDWR:
	    return MPI_MODE_RDWR;
	    break;
    case YogiMPI_MODE_WRONLY:
	    return MPI_MODE_WRONLY;
      	break;
    case YogiMPI_MODE_CREATE: 
	    return MPI_MODE_CREATE;
	    break;
    case YogiMPI_MODE_EXCL: 
	    return MPI_MODE_EXCL;
        break;
    case YogiMPI_MODE_DELETE_ON_CLOSE: 
	    return MPI_MODE_DELETE_ON_CLOSE;
	    break;
    case YogiMPI_MODE_UNIQUE_OPEN: 
	    return MPI_MODE_UNIQUE_OPEN;
	    break;
    case YogiMPI_MODE_SEQUENTIAL: 
	    return MPI_MODE_SEQUENTIAL;
	    break;
    case YogiMPI_MODE_APPEND:
	    return MPI_MODE_APPEND;
	    break;
    default:
    	return amode;
    }

	
}

/* Convert root constants in the case of an intercommunicator. */
static int root_to_mpi(int root) {
    if (root == YogiMPI_ROOT) return MPI_ROOT;
    if (root == YogiMPI_PROC_NULL) return MPI_PROC_NULL;
    return root;
}

/* Checks if an MPI_Datatype is present in existing datatype pool.
 * Returns index of location of datatype, or -1 if not found.
 */
static int check_datatype_presence(MPI_Datatype type_input) {
	int current = 0;
	while(current < num_datatypes) {
	    if (datatype_pool[current] == type_input) {
	        return current;
	    }
	    ++current;//
    }
    return -1;	
}

/* Checks if an MPI_Comm is present in existing comm pool.
 * Returns index of location of comm, or -1 if not found.
 */
static int check_comm_presence(MPI_Comm comm_input) {
    int current = 0;
    while(current < num_comms) {
        if (comm_pool[current] == comm_input) {
	    return current;
	}
	++current;//
    }
    return -1;	
}


/* Convert an MPI_Datatype array to a YogiMPI_Datatype array. This adds new
 * handles to the datatype pool. */
static void datatype_array_to_yogi(MPI_Datatype *inputArray,
                                   YogiMPI_Datatype *outputArray,
                                   int count) {
    int i;
    for (i = 0; i < count; i++) {
    	/* See if this datatype is already registered in the pool.  If so,
    	 * just give the existing index.  If not, add it to the pool.
    	 * This is needed to make MPI_Type_get_contents work properly.
    	 */
    	int location = check_datatype_presence(inputArray[i]);
    	if (location >= 0) {
    		outputArray[i] = location;
    	}
    	else {
            outputArray[i] = add_new_datatype(inputArray[i]);
    	}
    }
} 

static MPI_Aint * aint_array_to_mpi(YogiMPI_Aint in[], int count) {
	int i;
	MPI_Aint * conv_aint = (MPI_Aint *)malloc(count*sizeof(MPI_Aint));
	for (i = 0; i < count; i++) {
		conv_aint[i] = aint_to_yogi(in[i]);
	}
	return conv_aint;	
}

/* Convert an MPI_Aint array to a YogiMPI_Aint array. */
static void aint_array_to_yogi(MPI_Aint *inputArray, YogiMPI_Aint *outputArray,
                               int count) {
    int i;
    for (i = 0; i < count; i++) {
        outputArray[i] = aint_to_mpi(inputArray[i]);
    }
} 

static MPI_Offset * offset_array_to_mpi(YogiMPI_Offset in[], int count) {
	int i;
	MPI_Offset * conv_offset = (MPI_Offset *)malloc(count*sizeof(MPI_Offset));
	for (i = 0; i < count; i++) {
		conv_offset[i] = offset_to_mpi(in[i]);
	}
	return conv_offset;
}

static void free_aint_array(MPI_Aint * in) {
	if (in != NULL) {
	    free(in);
	}
}

static void free_offset_array(MPI_Offset * in) {
	if (in != NULL) {
	    free(in);
	}
}

YogiMPI_Comm fortran_comm_to_yogi(int fortran_comm) {
    MPI_Comm c_comm = MPI_Comm_f2c(fortran_comm);
    return add_new_comm(c_comm); 
}

YogiMPI_Group fortran_group_to_yogi(int fortran_group) {
    MPI_Group c_group = MPI_Group_f2c(fortran_group);
    return add_new_group(c_group);
}

YogiMPI_Comm Yogi_ResolveComm(void *commObject) {
    MPI_Comm *aComm = (MPI_Comm*)commObject;
    return check_comm_presence(*aComm);
}

YogiMPI_Comm Yogi_ResolveComm_f2c(int *fCommObject) {
    MPI_Comm cver = MPI_Comm_f2c(*fCommObject); 
    return Yogi_ResolveComm(&cver);
}

YogiMPI_Datatype Yogi_ResolveDatatype(void *datatypeObject) {
    MPI_Datatype *aType = (MPI_Datatype*)datatypeObject;
    return check_datatype_presence(*aType);
}

int Yogi_ResolveErrorcode(int errorcode) {
    return error_to_yogi(errorcode);
}

static void bind_mpi_err_constants() {

    mpi_error_codes[YogiMPI_SUCCESS]         = MPI_SUCCESS;
    mpi_error_codes[YogiMPI_ERR_BUFFER]      = MPI_ERR_BUFFER;
    mpi_error_codes[YogiMPI_ERR_COUNT]       = MPI_ERR_COUNT;
    mpi_error_codes[YogiMPI_ERR_TYPE]        = MPI_ERR_TYPE;
    mpi_error_codes[YogiMPI_ERR_TAG]         = MPI_ERR_TAG;
    mpi_error_codes[YogiMPI_ERR_COMM]        = MPI_ERR_COMM;
    mpi_error_codes[YogiMPI_ERR_RANK]        = MPI_ERR_RANK;
    mpi_error_codes[YogiMPI_ERR_REQUEST]     = MPI_ERR_REQUEST;
    mpi_error_codes[YogiMPI_ERR_ROOT]        = MPI_ERR_ROOT;
    mpi_error_codes[YogiMPI_ERR_GROUP]       = MPI_ERR_GROUP;
    mpi_error_codes[YogiMPI_ERR_OP]          = MPI_ERR_OP;
    mpi_error_codes[YogiMPI_ERR_TOPOLOGY]    = MPI_ERR_TOPOLOGY;
    mpi_error_codes[YogiMPI_ERR_DIMS]        = MPI_ERR_DIMS;
    mpi_error_codes[YogiMPI_ERR_ARG]         = MPI_ERR_ARG;
    mpi_error_codes[YogiMPI_ERR_UNKNOWN]     = MPI_ERR_UNKNOWN;
    mpi_error_codes[YogiMPI_ERR_TRUNCATE]    = MPI_ERR_TRUNCATE;
    mpi_error_codes[YogiMPI_ERR_OTHER]       = MPI_ERR_OTHER;
    mpi_error_codes[YogiMPI_ERR_INTERN]      = MPI_ERR_INTERN;
    mpi_error_codes[YogiMPI_ERR_PENDING]     = MPI_ERR_PENDING;
    mpi_error_codes[YogiMPI_ERR_IN_STATUS]   = MPI_ERR_IN_STATUS;
    mpi_error_codes[YogiMPI_ERR_FILE]        = MPI_ERR_FILE;
    mpi_error_codes[YogiMPI_ERR_NOT_SAME]    = MPI_ERR_NOT_SAME;
    mpi_error_codes[YogiMPI_ERR_AMODE]       = MPI_ERR_AMODE;
    mpi_error_codes[YogiMPI_ERR_UNSUPPORTED_DATAREP] 
                                              = MPI_ERR_UNSUPPORTED_DATAREP;
    mpi_error_codes[YogiMPI_ERR_UNSUPPORTED_OPERATION] 
                                              = MPI_ERR_UNSUPPORTED_OPERATION;
    mpi_error_codes[YogiMPI_ERR_NO_SUCH_FILE] = MPI_ERR_NO_SUCH_FILE;
    mpi_error_codes[YogiMPI_ERR_FILE_EXISTS]  = MPI_ERR_FILE_EXISTS;
    mpi_error_codes[YogiMPI_ERR_BAD_FILE]     = MPI_ERR_BAD_FILE;
    mpi_error_codes[YogiMPI_ERR_ACCESS]       = MPI_ERR_ACCESS; 
    mpi_error_codes[YogiMPI_ERR_NO_SPACE]     = MPI_ERR_NO_SPACE; 
    mpi_error_codes[YogiMPI_ERR_QUOTA]        = MPI_ERR_QUOTA; 
    mpi_error_codes[YogiMPI_ERR_READ_ONLY]    = MPI_ERR_READ_ONLY; 
    mpi_error_codes[YogiMPI_ERR_FILE_IN_USE]  = MPI_ERR_FILE_IN_USE; 
    mpi_error_codes[YogiMPI_ERR_DUP_DATAREP]  = MPI_ERR_DUP_DATAREP; 
    mpi_error_codes[YogiMPI_ERR_CONVERSION]   = MPI_ERR_CONVERSION; 
    mpi_error_codes[YogiMPI_ERR_IO]           = MPI_ERR_IO;
    mpi_error_codes[YogiMPI_ERR_LASTCODE]     = MPI_ERR_LASTCODE;

}

static void initialize_errhandler_pool() {
    int i;
    assert(!errhandler_pool);
  
    errhandler_pool = (MPI_Errhandler *)malloc(sizeof(MPI_Errhandler)*num_errhandlers);
    for(i = 0; i < num_errhandlers; ++i) {
    	errhandler_pool[i] = MPI_ERRHANDLER_NULL;
    }
    
	/* Predefined error handlers */
	errhandler_pool[YogiMPI_ERRHANDLER_NULL]  = MPI_ERRHANDLER_NULL;
	errhandler_pool[YogiMPI_ERRORS_ARE_FATAL] = MPI_ERRORS_ARE_FATAL;
	errhandler_pool[YogiMPI_ERRORS_RETURN]    = MPI_ERRORS_RETURN;
}

static void initialize_datatype_pool() {
    
    int i;
    assert(!datatype_pool);
    assert(num_datatypes > YogiMPI_PACKED);
  
    /* Initial allocation of datatype pool */
    datatype_pool = (MPI_Datatype *)malloc(sizeof(MPI_Datatype)*num_datatypes);
    for(i = 0; i < num_datatypes; ++i) {
    	datatype_pool[i] = MPI_DATATYPE_NULL;
    }
	datatype_pool[YogiMPI_CHAR]              = MPI_CHAR;
	datatype_pool[YogiMPI_SHORT]             = MPI_SHORT;
	datatype_pool[YogiMPI_INT]               = MPI_INT;
	datatype_pool[YogiMPI_LONG]              = MPI_LONG;
	datatype_pool[YogiMPI_UNSIGNED_CHAR]     = MPI_UNSIGNED_CHAR;
	datatype_pool[YogiMPI_UNSIGNED_SHORT]    = MPI_UNSIGNED_SHORT;
	datatype_pool[YogiMPI_UNSIGNED]          = MPI_UNSIGNED;
	datatype_pool[YogiMPI_UNSIGNED_LONG]     = MPI_UNSIGNED_LONG;
	datatype_pool[YogiMPI_FLOAT]             = MPI_FLOAT;
	datatype_pool[YogiMPI_DOUBLE]            = MPI_DOUBLE;
	datatype_pool[YogiMPI_LONG_DOUBLE]       = MPI_LONG_DOUBLE;
	datatype_pool[YogiMPI_BYTE]              = MPI_BYTE;
	datatype_pool[YogiMPI_PACKED]            = MPI_PACKED;
	datatype_pool[YogiMPI_FLOAT_INT]         = MPI_FLOAT_INT;
	datatype_pool[YogiMPI_DOUBLE_INT]        = MPI_DOUBLE_INT;
	datatype_pool[YogiMPI_LONG_INT]          = MPI_LONG_INT;
	datatype_pool[YogiMPI_2INT]              = MPI_2INT;
	datatype_pool[YogiMPI_SHORT_INT]         = MPI_SHORT_INT;
	datatype_pool[YogiMPI_LONG_DOUBLE_INT]   = MPI_LONG_DOUBLE_INT;
	datatype_pool[YogiMPI_LONG_LONG_INT]     = MPI_LONG_LONG_INT;
	datatype_pool[YogiMPI_INT32_T]           = MPI_INT32_T;
	datatype_pool[YogiMPI_INT64_T]           = MPI_INT64_T;

	datatype_pool[YogiMPI_COMPLEX]           = MPI_COMPLEX;
        datatype_pool[YogiMPI_DOUBLE_COMPLEX]    = MPI_DOUBLE_COMPLEX;
	datatype_pool[YogiMPI_LOGICAL]           = MPI_LOGICAL;
	datatype_pool[YogiMPI_2REAL]             = MPI_2REAL;
	datatype_pool[YogiMPI_2DOUBLE_PRECISION] = MPI_2DOUBLE_PRECISION; 
	datatype_pool[YogiMPI_2INTEGER]          = MPI_2INTEGER;
	datatype_pool[YogiMPI_INTEGER1]          = MPI_INTEGER1;
	datatype_pool[YogiMPI_INTEGER2]          = MPI_INTEGER2;
	datatype_pool[YogiMPI_INTEGER4]          = MPI_INTEGER4;
	datatype_pool[YogiMPI_INTEGER8]          = MPI_INTEGER8;
	datatype_pool[YogiMPI_REAL4]             = MPI_REAL4;
	datatype_pool[YogiMPI_REAL8]             = MPI_REAL8;
        datatype_pool[YogiMPI_UNSIGNED_LONG_LONG] = MPI_UNSIGNED_LONG_LONG;
        datatype_pool[YogiMPI_LB]                = MPI_LB;
        datatype_pool[YogiMPI_UB]                = MPI_UB;
	
}

static void initialize_group_pool() {
	
	int i;
    assert(!group_pool);
    group_pool = (MPI_Group *)malloc(sizeof(MPI_Group)*num_groups);
    for(i = 0; i < num_groups; ++i) group_pool[i] = MPI_GROUP_NULL;

    register_preindexed_group(YogiMPI_GROUP_NULL, MPI_GROUP_NULL);
    register_preindexed_group(YogiMPI_GROUP_EMPTY, MPI_GROUP_EMPTY);

}

static void initialize_comm_pool() {
    
    int i;
    assert(!comm_pool);
    comm_pool = (MPI_Comm *)malloc(sizeof(MPI_Comm)*num_comms);
    
    register_preindexed_comm(YogiMPI_COMM_NULL, MPI_COMM_NULL);
    register_preindexed_comm(YogiMPI_COMM_WORLD, MPI_COMM_WORLD);
    register_preindexed_comm(YogiMPI_COMM_SELF, MPI_COMM_SELF);	
}

static void initialize_request_pool() {
	int i;
    request_pool = (MPI_Request *)malloc(num_requests*sizeof(MPI_Request));
    for(i = 0; i < num_requests; ++i) request_pool[i] = MPI_REQUEST_NULL;	
}

static void initialize_file_pool() {
	int i;
    file_pool = (MPI_File *)malloc(num_files*sizeof(MPI_File));
    for(i = 0; i < num_files; ++i) file_pool[i] = MPI_FILE_NULL;	
}

static void initialize_info_pool() {
	int i;
    info_pool = (MPI_Info *)malloc(num_infos*sizeof(MPI_Info));
    for(i = 0; i < num_infos; ++i) info_pool[i] = MPI_INFO_NULL;	
}

static void initialize_op_pool() {
	int i;
    assert(num_ops > YogiMPI_OP_VOLATILE_OFFSET);
    op_pool = (MPI_Op *)malloc(num_ops*sizeof(MPI_Op));
    for(i = 0; i < num_ops; ++i ) op_pool[i] = MPI_OP_NULL;
    op_pool[YogiMPI_MAX]    = MPI_MAX;
    op_pool[YogiMPI_MIN]    = MPI_MIN;
    op_pool[YogiMPI_SUM]    = MPI_SUM;
    op_pool[YogiMPI_PROD]   = MPI_PROD;
    op_pool[YogiMPI_MAXLOC] = MPI_MAXLOC;
    op_pool[YogiMPI_MINLOC] = MPI_MINLOC;
    op_pool[YogiMPI_BAND]   = MPI_BAND;
    op_pool[YogiMPI_BOR]    = MPI_BOR;
    op_pool[YogiMPI_BXOR]   = MPI_BXOR;
    op_pool[YogiMPI_LAND]   = MPI_LAND;
    op_pool[YogiMPI_LOR]    = MPI_LOR;
    op_pool[YogiMPI_LXOR]   = MPI_LXOR;	
}

void allocate_yogimpi_storage() {

    /* mpich2/test/mpi/pt2pt/bottom.c fails otherwise so there is soth. fishy */
    assert(0 == MPI_BOTTOM);
    assert(YogiMPI_BSEND_OVERHEAD >= MPI_BSEND_OVERHEAD);
    
    /* Initialize the back-end arrays for opaque objects and references */
    bind_mpi_err_constants();
    if (datatype_pool == NULL) initialize_datatype_pool();
    if (errhandler_pool == NULL) initialize_errhandler_pool();
    if (group_pool == NULL) initialize_group_pool();
    if (comm_pool == NULL) initialize_comm_pool();
    if (request_pool == NULL) initialize_request_pool();
    if (op_pool == NULL) initialize_op_pool();
    if (info_pool == NULL) initialize_info_pool();
    if (file_pool == NULL) initialize_file_pool();

}

void deallocate_yogimpi_storage(void) {

    if (request_pool != NULL) {
        free(request_pool);
        request_pool = NULL;
    }
    if (comm_pool != NULL) {
        free(comm_pool);
        comm_pool = NULL;
    }
    if (group_pool != NULL) {
        free(group_pool);
        group_pool = NULL;
    }
    if (info_pool != NULL) {
        free(info_pool);
        info_pool = NULL;
    }
    if (file_pool != NULL) {
        free(file_pool);
        file_pool = NULL;
    }
    if (op_pool != NULL) {
        free(op_pool);
        op_pool = NULL;
    }
    if (datatype_pool != NULL) {
        free(datatype_pool);
        datatype_pool = NULL;
    }
    if (errhandler_pool != NULL) {
        free(errhandler_pool);
    	errhandler_pool = NULL;
    }
}

int YogiMPI_Init(int* argc, char ***argv)
{ 

    allocate_yogimpi_storage();

    int mpi_err = MPI_Init(argc,argv); 

    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
 
    return error_to_yogi(mpi_err);
}

// Begin automatically-generated function code.
@YOGI_FUNCTIONS@
// End automatically-generated function code.
