/*
                                  COPYRIGHT

 The following is a notice of limited availability of the code, and disclaimer
 which must be included in the prologue of the code and in all source listings
 of the code.

 Copyright Notice
  + 2002 University of Chicago
  + 2016 Stephen Adamec

 Permission is hereby granted to use, reproduce, prepare derivative works, and
 to redistribute to others.  This software was authored by:

 Mathematics and Computer Science Division
 Argonne National Laboratory, Argonne IL 60439

 (and)

 Department of Computer Science
 University of Illinois at Urbana-Champaign

 (and)

 Stephen Adamec

                              GOVERNMENT LICENSE

 Portions of this material resulted from work developed under a U.S.
 Government Contract and are subject to the following license: the Government
 is granted for itself and others acting on its behalf a paid-up, nonexclusive,
 irrevocable worldwide license in this computer software to reproduce, prepare
 derivative works, and perform publicly and display publicly.

                                    DISCLAIMER

 This computer code material was prepared, in part, as an account of work
 sponsored by an agency of the United States Government.  Neither the United
 States, nor the University of Chicago, nor any of their employees, makes any
 warranty express or implied, or assumes any legal liability or responsibility
 for the accuracy, completeness, or usefulness of any information, apparatus,
 product, or process disclosed, or represents that its use would not infringe
 privately owned rights.
*/

#include "YogiManager.h"
#include <cstring>

int Yogi_ResolveErrorCode(int *error) {
    return YogiManager::getInstance()->errorToYogi(*error);
}

YogiMPI_Datatype Yogi_ResolveDatatype(void *input_pointer) {
    MPI_Datatype *to_resolve = reinterpret_cast<MPI_Datatype *>(input_pointer);
    return YogiManager::getInstance()->datatypeToYogi(*to_resolve);
}

YogiMPI_Comm Yogi_ResolveComm(void *input_pointer) {
    MPI_Comm *to_resolve = reinterpret_cast<MPI_Comm *>(input_pointer);
    return YogiManager::getInstance()->commToYogi(*to_resolve);
}

YogiMPI_Comm Yogi_ResolveFortranComm(int fortran_comm) {
    MPI_Comm c_comm = MPI_Comm_f2c(fortran_comm);
    return YogiManager::getInstance()->commToYogi(c_comm);
}

YogiMPI_Group Yogi_ResolveFortranGroup(int fortran_group) {
    MPI_Group c_group = MPI_Group_f2c(fortran_group);
    return YogiManager::getInstance()->groupToYogi(c_group);
}

YogiMPI_Offset Yogi_ResolveOffset(void *input_pointer) {
    MPI_Offset *to_resolve = reinterpret_cast<MPI_Offset *>(input_pointer);
    return YogiManager::getInstance()->aintToYogi(*to_resolve);
}

YogiMPI_Win Yogi_ResolveWin(void *input_pointer) {
    MPI_Win *to_resolve = reinterpret_cast<MPI_Win *>(input_pointer);
    return YogiManager::getInstance()->winToYogi(*to_resolve);
}

YogiMPI_File Yogi_ResolveFile(void *input_pointer) {
    MPI_File *to_resolve = reinterpret_cast<MPI_File *>(input_pointer);
    return YogiManager::getInstance()->fileToYogi(*to_resolve);
}

YogiMPI_Aint Yogi_ResolveAint(void *input_pointer) {
    MPI_Aint *to_resolve = reinterpret_cast<MPI_Aint *>(input_pointer);
    return YogiManager::getInstance()->aintToYogi(*to_resolve);
}

int YogiMPI_Comm_spawn(const char *command, char *argv[], int maxprocs, YogiMPI_Info info, int root, YogiMPI_Comm comm, YogiMPI_Comm *intercomm, int array_of_errcodes[]) {
    MPI_Info mInfo = YogiManager::getInstance()->infoToMPI(info);
    MPI_Comm mComm = YogiManager::getInstance()->commToMPI(comm);
    MPI_Comm mInterComm = YogiManager::getInstance()->commToMPI(*intercomm);
    YogiManager::getInstance()->callDepth++;
    int mpi_err = MPI_Comm_spawn(command, argv, maxprocs, mInfo, root, mComm, &mInterComm, array_of_errcodes);
    YogiManager::getInstance()->callDepth--;
    return YogiManager::getInstance()->errorToYogi(mpi_err);
}


int YogiMPI_Init(int* argc, char ***argv)
{
    YogiManager::getInstance()->loadMPILibrary();
    YogiManager::getInstance()->callDepth++;
    int mpi_err = MPI_Init(argc, argv);
    YogiManager::getInstance()->callDepth--;
#ifdef YOGI_DEBUG
    int glob_rank = -1;
    YogiManager::getInstance()->callDepth++;
    MPI_Comm_rank(MPI_COMM_WORLD, &glob_rank);
    YogiManager::getInstance()->callDepth--;
    YogiManager::getInstance()->setGlobalRank(glob_rank);
    YogiManager::getInstance()->openDebugLog();
    YogiManager::getInstance()->writeToDebugLog("Completed MPI_Init.");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_err);
}

int YogiMPI_Init_thread(int* argc, char*** argv, int required, int* provided) {
    int mpi_error;
    YogiManager::getInstance()->loadMPILibrary();

    required = YogiManager::getInstance()->threadmodelToMPI(required);
    YogiManager::getInstance()->callDepth++;
    mpi_error = MPI_Init_thread(argc, argv, required, provided);
    YogiManager::getInstance()->callDepth--;
    *provided = YogiManager::getInstance()->providedToYogi(*provided);
#ifdef YOGI_DEBUG
    int glob_rank = -1;
    YogiManager::getInstance()->callDepth++;
    MPI_Comm_rank(MPI_COMM_WORLD, &glob_rank);
    YogiManager::getInstance()->callDepth--;
    YogiManager::getInstance()->setGlobalRank(glob_rank);
    YogiManager::getInstance()->openDebugLog();
    YogiManager::getInstance()->writeToDebugLog("Completed MPI_Init_thread.");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_error);
}

int YogiMPI_Error_class(int errorcode, int* errorclass) {

#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering MPI_Error_class");
#endif

    /* We make an assumption that error codes line up with classes. In the
       case of implementation-specific error codes that do not, this must
       be revised. */
    *errorclass = errorcode;
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting MPI_Error_class");
#endif
    return YogiManager::getInstance()->errorToYogi(MPI_SUCCESS);
}

int YogiMPI_Finalize() {
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering MPI_Finalize");
#endif
    // NOTE: Incrementing/decrementing "callDepth" around MPI_Finalize breaks
    //       some things in some client unit tests.
    int mpi_err = MPI_Finalize();
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting MPI_Finalize");
    YogiManager::getInstance()->closeDebugLog();
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_err);
}

int YogiMPI_Get_processor_name(char* name, int* resultlen) {
    int mpi_error;
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering MPI_Get_processor_name");
#endif
    if (MPI_MAX_PROCESSOR_NAME > YogiMPI_MAX_PROCESSOR_NAME) {
        char *tmp_name = new char[MPI_MAX_PROCESSOR_NAME];
        YogiManager::getInstance()->callDepth++;
        mpi_error = MPI_Get_processor_name(tmp_name, resultlen);
        YogiManager::getInstance()->callDepth--;
        std::memcpy(name, tmp_name, YogiMPI_MAX_PROCESSOR_NAME - 1);
        name[YogiMPI_MAX_PROCESSOR_NAME - 1] = '\0';
        delete[] tmp_name;
        if (*resultlen > YogiMPI_MAX_PROCESSOR_NAME - 1) {
            *resultlen = YogiMPI_MAX_PROCESSOR_NAME - 1;
        }
    }
    else {
        YogiManager::getInstance()->callDepth++;
        mpi_error = MPI_Get_processor_name(name, resultlen);
        YogiManager::getInstance()->callDepth--;
    }
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting MPI_Get_processor_name");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_error);
}

int YogiMPI_Error_string(int errorcode, char* string, int* resultlen) {
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering YogiMPI_Error_string");
#endif
    int mpi_error;
    errorcode = YogiManager::getInstance()->errorToMPI(errorcode);
    if (MPI_MAX_ERROR_STRING > YogiMPI_MAX_ERROR_STRING) {
        char *tmp_string = new char[MPI_MAX_ERROR_STRING];
        YogiManager::getInstance()->callDepth++;
        mpi_error = MPI_Error_string(errorcode, tmp_string, resultlen);
        YogiManager::getInstance()->callDepth--;
        if (*resultlen > YogiMPI_MAX_ERROR_STRING - 1) {
            std::memcpy(string, tmp_string, YogiMPI_MAX_ERROR_STRING - 1);
            *resultlen = YogiMPI_MAX_ERROR_STRING - 1;
            string[YogiMPI_MAX_ERROR_STRING - 1] = '\0';
        }
        else {
            std::memcpy(string, tmp_string, *resultlen);
            string[*resultlen] = '\0';
        }
        delete[] tmp_string;
    }
    else {
        YogiManager::getInstance()->callDepth++;
        mpi_error = MPI_Error_string(errorcode, string, resultlen);
        YogiManager::getInstance()->callDepth--;
    }
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting YogiMPI_Error_string");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_error);
}

int YogiMPI_Type_get_contents(YogiMPI_Datatype datatype, int max_integers,
                              int max_addresses, int max_datatypes,
                              int array_of_integers[],
                              YogiMPI_Aint array_of_addresses[],
                              YogiMPI_Datatype array_of_datatypes[]) {

#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering MPI_Type_get_contents");
#endif
    int mpi_error;
    MPI_Datatype conv_datatype;
    conv_datatype = YogiManager::getInstance()->datatypeToMPI(datatype);

    /* In order to appropriately handle this function, we must call
     * MPI_Type_get_envelope to figure out what the user will get. This helps
     * determine if the type is derived and if new datatype handles are added
     * to our translation pool.
     */
    int num_integers, num_addresses, num_datatypes, combiner;
    YogiManager::getInstance()->callDepth++;
    MPI_Type_get_envelope(conv_datatype, &num_integers, &num_addresses,
                          &num_datatypes, &combiner);
    YogiManager::getInstance()->callDepth--;

    MPI_Aint * conv_array_of_addresses = new MPI_Aint[max_addresses];
    MPI_Datatype * conv_array_of_datatypes = new MPI_Datatype[max_datatypes];
    YogiManager::getInstance()->callDepth++;
    mpi_error = MPI_Type_get_contents(conv_datatype, max_integers,
                                      max_addresses, max_datatypes,
                                      array_of_integers,
                                      conv_array_of_addresses,
                                      conv_array_of_datatypes);
    YogiManager::getInstance()->callDepth--;
    if (combiner != MPI_COMBINER_F90_COMPLEX &&
        combiner != MPI_COMBINER_F90_REAL &&
        combiner != MPI_COMBINER_F90_INTEGER) {
        /* The array won't be empty, so convert them to Yogi versions. */
        YogiManager::getInstance()->datatypeToYogi(conv_array_of_datatypes,
                                                   array_of_datatypes,
                                                   max_datatypes, true);
    }
    else delete[] conv_array_of_datatypes;

    YogiManager::getInstance()->aintToYogi(conv_array_of_addresses, array_of_addresses, max_addresses, true);
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting MPI_Type_get_contents");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_error);
}

int YogiMPI_Type_create_darray(int size, int rank, int ndims, int array_of_gsizes[], int array_of_distribs[], int array_of_dargs[], int array_of_psizes[], int order, YogiMPI_Datatype oldtype, YogiMPI_Datatype* newtype) {
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Entering MPI_Type_create_darray");
#endif

    int mpi_error;
    MPI_Datatype conv_oldtype;
    conv_oldtype = YogiManager::getInstance()->datatypeToMPI(oldtype);
    MPI_Datatype conv_newtype;

    /* Constant conversion requires some temporary arrays to hold state
     * and distribution values.
     */
    int *distribs_conv = new int[ndims];
    int *dargs_conv = new int[ndims];
    int i;
    for (i = 0; i < ndims; i++) {
        switch(array_of_distribs[i]) {
            case YogiMPI_DISTRIBUTE_BLOCK:
                distribs_conv[i] = MPI_DISTRIBUTE_BLOCK;
                break;
            case YogiMPI_DISTRIBUTE_CYCLIC:
                distribs_conv[i] = MPI_DISTRIBUTE_CYCLIC;
                    break;
            case YogiMPI_DISTRIBUTE_NONE:
                distribs_conv[i] = MPI_DISTRIBUTE_NONE;
                    break;
            default:
                    distribs_conv[i] = array_of_distribs[i];
        }
        if (array_of_dargs[i] == YogiMPI_DISTRIBUTE_DFLT_DARG) {
            dargs_conv[i] = MPI_DISTRIBUTE_DFLT_DARG;
        }
        else {
                dargs_conv[i] = array_of_dargs[i];
        }
    }

    if (order == YogiMPI_ORDER_C) {
        order = MPI_ORDER_C;
    }
    else if(order == YogiMPI_ORDER_FORTRAN) {
        order = MPI_ORDER_FORTRAN;
    }

    YogiManager::getInstance()->callDepth++;
    mpi_error = MPI_Type_create_darray(size, rank, ndims, array_of_gsizes, distribs_conv, dargs_conv, array_of_psizes, order, conv_oldtype, &conv_newtype);
    YogiManager::getInstance()->callDepth--;
    *newtype = YogiManager::getInstance()->datatypeToYogi(conv_newtype);

    delete[] distribs_conv;
    delete[] dargs_conv;
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Exiting MPI_Type_create_darray");
#endif
    return YogiManager::getInstance()->errorToYogi(mpi_error);
}

double YogiMPI_Wtick() {
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Enter/exit MPI_Wtick");
#endif
    return MPI_Wtick();
}

double YogiMPI_Wtime() {
#ifdef YOGI_DEBUG
    YogiManager::getInstance()->writeToDebugLog("Enter/exit MPI_Wtime");
#endif
    return MPI_Wtime();
}

/* Converter functions between Fortran and C - really not needed since we
   have a single ABI, but provided. */

YogiMPI_Fint YogiMPI_Comm_c2f(YogiMPI_Comm comm) {
    return comm;
}

YogiMPI_Comm YogiMPI_Comm_f2c(YogiMPI_Fint comm) {
    return comm;
}

YogiMPI_Fint YogiMPI_Errhandler_c2f(YogiMPI_Errhandler err) {
    return err;
}

YogiMPI_Errhandler YogiMPI_Errhandler_f2c(YogiMPI_Fint err) {
    return err;
}

YogiMPI_Fint YogiMPI_File_c2f(YogiMPI_File file) {
    return file;
}

YogiMPI_File YogiMPI_File_f2c(YogiMPI_Fint file) {
    return file;
}

YogiMPI_Fint YogiMPI_Group_c2f(YogiMPI_Group group) {
    return group;
}

YogiMPI_Group YogiMPI_Group_f2c(YogiMPI_Fint group) {
    return group;
}

YogiMPI_Fint YogiMPI_Info_c2f(YogiMPI_Info info) {
    return info;
}

YogiMPI_Info YogiMPI_Info_f2c(YogiMPI_Fint info) {
    return info;
}

YogiMPI_Fint YogiMPI_Op_c2f(YogiMPI_Op op) {
    return op;
}

YogiMPI_Op YogiMPI_Op_f2c(YogiMPI_Fint op) {
    return op;
}

YogiMPI_Fint YogiMPI_Request_c2f(YogiMPI_Request request) {
    return request;
}

YogiMPI_Request YogiMPI_Request_f2c(YogiMPI_Fint request) {
    return request;
}

int YogiMPI_Status_f2c(const YogiMPI_Fint *f_status, YogiMPI_Status *c_status) {
    /* Do nothing - these are already equivalent. */
    return YogiManager::getInstance()->errorToYogi(MPI_SUCCESS);
}

int YogiMPI_Status_c2f(const YogiMPI_Status *c_status, YogiMPI_Fint *f_status) {
    /* Do nothing - these are already equivalent. */
    return YogiManager::getInstance()->errorToYogi(MPI_SUCCESS);
}

YogiMPI_Fint YogiMPI_Type_c2f(YogiMPI_Datatype datatype) {
    return datatype;
}

YogiMPI_Datatype YogiMPI_Type_f2c(YogiMPI_Fint datatype) {
    return datatype;
}

YogiMPI_Fint YogiMPI_Win_c2f(YogiMPI_Win win) {
    return win;
}

YogiMPI_Win YogiMPI_Win_f2c(YogiMPI_Fint win) {
    return win;
}


// Begin automatically-generated function code.
@YOGI_FUNCTIONS@
// End automatically-generated function code.
